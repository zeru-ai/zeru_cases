{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Machine Learning model to find out Investor Presentations  \n",
    "The investor presentation materials have great in-depth information but it also well highlights the high level pictures, instead of focusing on accounting details and disclousures required by regulations. Here is [an example of it](http://edgar.secdatabase.com/2259/115752318001602/filing-main.htm). \n",
    "\n",
    "However, these presentation slides are hidden among all the other SEC filings. No one can find it out if investor go check companies one by one. \n",
    "\n",
    "To make this finding easier, SECdatabase applies a Machine Learning algorithm to determine if one particular file is a investor presentations or not.\n",
    "\n",
    "This notebook is to share how we tackel tihs problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import SecdbAPI\n",
    "import pandas as pd\n",
    "import MyUtil\n",
    "import re\n",
    "import random\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from multiprocessing import Manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we read data from database, to help users understand the dataset, we provide a file-based snapshot, under the file 'data/8k_df.csv' which was dumped from pandas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(69623, 5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readFromDB=False\n",
    "if readFromDB:\n",
    "    df = SecdbAPI.load8K(12)\n",
    "    df.to_csv(\"data/8k_df.csv\", sep=\"\\t\", header=True)\n",
    "else:\n",
    "    df = pd.read_csv(\"data/8k_df.csv\",sep=\"\\t\")\n",
    "    df= df.drop(columns=[\"Unnamed: 0\"])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "As an initial step, to find out what features we can use to detect presentations in 8-k filines. we first come up a list after we mannually check some files. Here is a [smaple file list](http://edgar.secdatabase.com/886/119312517334408/filelist.txt).\n",
    "- number of files \n",
    "- number of pictures\n",
    "- size of the core html\n",
    "- average size of all pictures  \n",
    "All the features come from the file list. However, we beliece that the text in html could give us some more insights helping us make the judgement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add text based feature\n",
    "We know that if a filing is a presentation, they usually have the word \"presentation\" in the file text and also the categorized items is \"Item 7.01\", or \"Regulation FD Disclosure\". so we try to count how many times these phrases showing up in the documemt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchKeyWords(url):\n",
    "    cont = MyUtil.requestContent(url)\n",
    "    txt = cont.decode('ascii').lower()\n",
    "    \n",
    "    matches = txt.count(\"presentation\")+txt.count(\"meeting\")+\\\n",
    "            txt.count(\"webinar\")+txt.count(\"webcast\")\n",
    "    matches2 =  txt.count(\"regulation fd disclosure\")\n",
    "    \n",
    "    matches3 =  txt.count(\"earning\")\n",
    "    \n",
    "    return (matches , matches2 , matches3 )\n",
    "def extractFeature(urlt):\n",
    "    try:\n",
    "        cont = MyUtil.requestContent(urlt+'filelist.txt')\n",
    "        ss = cont.decode('ascii')\n",
    "        lines=ss.split('\\r')\n",
    "        num_files=len(lines)\n",
    "        size_corehtml=int(lines[0].split(\"\\t\")[3])//1000\n",
    "        keyfile=lines[0].split('\\t')[2]\n",
    "        kw1, kw2, kw3 = searchKeyWords(urlt+keyfile)\n",
    "        graphs=[l for l in lines if len(l.strip())!=0 and l.split(\"\\t\")[1]==\"GRAPHIC\"]\n",
    "        num_pict = len(graphs)\n",
    "        avg_pict_size =sum([int(a.split(\"\\t\")[3]) for a in graphs])//(num_pict*1000) \\\n",
    "                            if num_pict!=0 else 0\n",
    "\n",
    "        return \"\\t\".join([urlt+'filelist.txt', str(num_files), str(size_corehtml), \\\n",
    "                str(num_pict), str(avg_pict_size), str(kw1), str(kw2), str(kw3)])+\"\\r\"\n",
    "    except:\n",
    "        print(\"failed\")\n",
    "        return \"failed\"+urlt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use Python's ThreadPool library to give us a hand on speeding up the program. Because of Python's [Global Interpreter Lock](https://wiki.python.org/moin/GlobalInterpreterLock), python doesn't really have multithreading per se, but, as this downloading spends most of the time in waiting network I/O, multi-threading still helps us a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls=df.loc[:,'url']\n",
    "\n",
    "pool = ThreadPool(processes=20)\n",
    "output=pool.map(extractFeature, urls)\n",
    "\n",
    "with open(\"data/8k_present3.txt\", \"w\") as ff:\n",
    "    ff.write(\"\".join(output))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually label our data\n",
    "As a classification problem, we need labeled data to train our model, so we have to label them mannually. \n",
    "Our thoughts are firstly we focus on the boundary, for those files that contain 4 to 7 pictures, which could be a presentation or a random 8-K report having several scanned document images. Here is an example of such reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.utils \n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error,\\\n",
    "    r2_score,classification_report,precision_recall_fscore_support\n",
    "\n",
    "data=pd.read_csv(\"data/8k_present_labeled.txt\",sep=\"\\t\",header=0)\n",
    "data = sklearn.utils.shuffle(data)\n",
    "cut=int(0.8*len(data))\n",
    "train=data.head(cut)\n",
    "valid=data.tail(len(data)-cut)\n",
    "intputs=[\"size_corehtml\",\"num_pict\",\"avg_pict_size\",\"kw1\",\"kw2\",\"kw3\"]\n",
    "train_x= train[intputs]\n",
    "train_y= train[\"IsPresentation\"]\n",
    "valid_x=valid[intputs]\n",
    "valid_y=valid[\"IsPresentation\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our testing, we found `Number of Files` is not a good indication, so we exclude it from our features.  \n",
    "Like all the Machine Learning project, finding the right feature, aka Feature Engineering, is an interactive process and takes a decent amount of time. To make this demo concise, we don't include our feature selection steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: \n",
      " [[-1.17231602e-02  1.09428410e-02  7.55687369e-04  5.24685845e-01\n",
      "   1.96332677e+00  2.24943295e-01]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create linear regression object\n",
    "regr = linear_model.LogisticRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(train_x, train_y)\n",
    "\n",
    "# The coefficients\n",
    "print('Coefficients: \\n', regr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficients are the outcome of the whole model:  \n",
    "`Coefficients: \n",
    " [[-1.17231602e-02  1.09428410e-02  7.55687369e-04  5.24685845e-01  1.96332677e+00  2.24943295e-01]]\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-sample metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 0.11\n",
      "Variance score: 0.42\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.70      0.76       169\n",
      "          1       0.90      0.95      0.93       487\n",
      "\n",
      "avg / total       0.89      0.89      0.89       656\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions using the testing set\n",
    "pred_y = regr.predict(train_x)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(pred_y, train_y))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(train_y, pred_y))\n",
    "\n",
    "pred_y3=[1 if x>0.5 else 0 for x in pred_y]\n",
    "print(classification_report(train_y, pred_y3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-Sample validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 0.15\n",
      "Variance score: 0.26\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.60      0.69        45\n",
      "          1       0.86      0.95      0.90       119\n",
      "\n",
      "avg / total       0.85      0.85      0.85       164\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions using the testing set\n",
    "pred_y = regr.predict(valid_x)\n",
    "# The mean squared error\n",
    "print(\"Mean squared error: %.2f\"\n",
    "      % mean_squared_error(pred_y, valid_y))\n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('Variance score: %.2f' % r2_score(valid_y, pred_y))\n",
    "\n",
    "pred_y2=[1 if x>0.5 else 0 for x in pred_y]\n",
    "print(classification_report(valid_y, pred_y2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a look at the incorrect predictions\n",
    "This step helps us for two things. We can find out where our model predicts incorrectly, so we have intuition to adjust our model. Meanwhile, our manually labeled data may have some errors, so we can make our data better quality, which will further enhance our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>num_files</th>\n",
       "      <th>size_corehtml</th>\n",
       "      <th>num_pict</th>\n",
       "      <th>avg_pict_size</th>\n",
       "      <th>kw1</th>\n",
       "      <th>kw2</th>\n",
       "      <th>kw3</th>\n",
       "      <th>IsPresentation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>http://edgar.secdatabase.com/946/1493152180094...</td>\n",
       "      <td>55</td>\n",
       "      <td>28</td>\n",
       "      <td>52</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>http://edgar.secdatabase.com/893/1279569180007...</td>\n",
       "      <td>114</td>\n",
       "      <td>175</td>\n",
       "      <td>100</td>\n",
       "      <td>121</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>http://edgar.secdatabase.com/493/1144204180055...</td>\n",
       "      <td>29</td>\n",
       "      <td>1150</td>\n",
       "      <td>19</td>\n",
       "      <td>61</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>http://edgar.secdatabase.com/2219/100210518000...</td>\n",
       "      <td>94</td>\n",
       "      <td>83</td>\n",
       "      <td>88</td>\n",
       "      <td>335</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>http://edgar.secdatabase.com/2199/915761800001...</td>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "      <td>25</td>\n",
       "      <td>114</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>555</th>\n",
       "      <td>http://edgar.secdatabase.com/2939/875159180000...</td>\n",
       "      <td>48</td>\n",
       "      <td>32</td>\n",
       "      <td>44</td>\n",
       "      <td>146</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   URL  num_files  \\\n",
       "213  http://edgar.secdatabase.com/946/1493152180094...         55   \n",
       "740  http://edgar.secdatabase.com/893/1279569180007...        114   \n",
       "164  http://edgar.secdatabase.com/493/1144204180055...         29   \n",
       "644  http://edgar.secdatabase.com/2219/100210518000...         94   \n",
       "159  http://edgar.secdatabase.com/2199/915761800001...         30   \n",
       "555  http://edgar.secdatabase.com/2939/875159180000...         48   \n",
       "\n",
       "     size_corehtml  num_pict  avg_pict_size  kw1  kw2  kw3  IsPresentation  \n",
       "213             28        52             92    0    0    0               1  \n",
       "740            175       100            121    2    0    0               1  \n",
       "164           1150        19             61   24    0    6               1  \n",
       "644             83        88            335    2    0    0               1  \n",
       "159             32        25            114    1    0    1               1  \n",
       "555             32        44            146    2    0    0               1  "
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#false negative, error in recall\n",
    "diff=[y1==1 and y2==0 for y1, y2 in zip(valid_y, pred_y2)]\n",
    "valid[diff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size_corehtml</th>\n",
       "      <th>num_pict</th>\n",
       "      <th>avg_pict_size</th>\n",
       "      <th>kw1</th>\n",
       "      <th>kw2</th>\n",
       "      <th>kw3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>36</td>\n",
       "      <td>15</td>\n",
       "      <td>81</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>28</td>\n",
       "      <td>36</td>\n",
       "      <td>119</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>17</td>\n",
       "      <td>22</td>\n",
       "      <td>272</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>231</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>44</td>\n",
       "      <td>68</td>\n",
       "      <td>382</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>22</td>\n",
       "      <td>14</td>\n",
       "      <td>209</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>147</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>31</td>\n",
       "      <td>35</td>\n",
       "      <td>172</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>37</td>\n",
       "      <td>21</td>\n",
       "      <td>127</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>15</td>\n",
       "      <td>35</td>\n",
       "      <td>86</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>259</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>40</td>\n",
       "      <td>16</td>\n",
       "      <td>163</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>38</td>\n",
       "      <td>57</td>\n",
       "      <td>201</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>27</td>\n",
       "      <td>36</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>365</td>\n",
       "      <td>31</td>\n",
       "      <td>234</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>48</td>\n",
       "      <td>14</td>\n",
       "      <td>163</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>237</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>25</td>\n",
       "      <td>21</td>\n",
       "      <td>105</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626</th>\n",
       "      <td>35</td>\n",
       "      <td>43</td>\n",
       "      <td>62</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>18</td>\n",
       "      <td>36</td>\n",
       "      <td>205</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>32</td>\n",
       "      <td>24</td>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>30</td>\n",
       "      <td>24</td>\n",
       "      <td>143</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     size_corehtml  num_pict  avg_pict_size  kw1  kw2  kw3\n",
       "295             36        15             81    3    0    0\n",
       "49              28        36            119    2    0    0\n",
       "234             17        22            272    2    0    0\n",
       "85              16        32            231    0    0    0\n",
       "709             44        68            382    1    0    0\n",
       "410             22        14            209    3    0    0\n",
       "166             20        19            147    0    0    2\n",
       "103             31        35            172    2    0    0\n",
       "311             37        21            127    3    0    0\n",
       "102             15        35             86    2    0    0\n",
       "181             14        18            259    1    0    0\n",
       "61              40        16            163    3    0    0\n",
       "206             38        57            201    1    0    0\n",
       "47              27        36            121    0    0    0\n",
       "397            365        31            234    4    0    0\n",
       "412             48        14            163    3    0    0\n",
       "67              14        16            237    1    0    0\n",
       "306             25        21            105    3    0    0\n",
       "626             35        43             62    2    0    2\n",
       "44              18        36            205    1    0    0\n",
       "348             32        24            128    2    0    0\n",
       "343             30        24            143    0    0    2"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#false negative, error in recall\n",
    "diff=[y1==1 and y2==0 for y1, y2 in zip(train_y, pred_y3)]\n",
    "train_x[diff]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Random Forest method\n",
    "After looking at all the failed cases in regression model, the performance is not okay but not great, especially lack of great precision. Random Forest is built on top of a number of small decision trees, which can handle outlier and non-linear relationship pretty well. We give it a try here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=3, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=False, random_state=0,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=3, random_state=0)\n",
    "clf.fit(train_x, train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.93      0.95       169\n",
      "          1       0.98      0.99      0.98       487\n",
      "\n",
      "avg / total       0.98      0.98      0.98       656\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_y= clf.predict(train_x)\n",
    "print(classification_report(train_y, pred_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      0.89      0.93        45\n",
      "          1       0.96      0.99      0.98       119\n",
      "\n",
      "avg / total       0.96      0.96      0.96       164\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['http://edgar.secdatabase.com/946/149315218009498/filelist.txt']"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_y2= clf.predict(valid_x)\n",
    "print(classification_report(valid_y, pred_y2))\n",
    "#false negative, error in recall\n",
    "diff=[y1==1 and y2==0 for y1, y2 in zip(valid_y, pred_y2)]\n",
    "valid[diff]['URL'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After tuning the model parameters, we came up with a pretty accurate model, which has better performance than Logistic Regression. This result is not surprising at all. However, Random Forest model has its downside. A prediction by linear regression model can be easily implemented in any language without using external library, since it is nothing by a weighted average of each input; logistic regression adds a tanh or sigmoid layer at the end, but still pretty straightforward. Random Forest is a discriminent model, and the prediction is the average or votes of a number of small trees, so it is not trivial to implement the algorithm and also has to load the trees data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict for all the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat=pd.read_csv(\"data/8k_present3.txt\",sep=\"\\t\", names=\\\n",
    "        [\"num_files\",\"size_corehtml\",\"num_pict\",\"avg_pict_size\",\"kw1\",\"kw2\",\"kw3\"])\n",
    "predict_y = clf.predict(dat[intputs])\n",
    "dat[\"Prediction\"]=predict_y\n",
    "\n",
    "dat[dat[\"Prediction\"]==1].to_csv(\"data/8k_presentation_predicts.csv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the final result  \n",
    "After doing all these modeling, we had the final files that contains the presentations. However, there is still a final step left, to find out the entry URL that let the viewers go directly to the presentation.  \n",
    "Our approach is pretty simple. If there is a PDF file, we use it; then we trying to find out among all the htm files, which one contrains the picture names in the HTML body. That must be the right one.  \n",
    "At last, we save it in a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detechString(urlhtm, substr_list):\n",
    "    content = MyUtil.requestContent(urlhtm)\n",
    "    string =content.decode('utf-8', 'ignore')\n",
    "    return sum([string.count(s) for s in substr_list])\n",
    "    \n",
    "\n",
    "urlz= dat[dat[\"Prediction\"]==1].index.tolist()\n",
    "res =set()\n",
    "for urlt in urlz:\n",
    "    urlp= urlt.replace('/filelist.txt','')\n",
    "    cont = MyUtil.requestContent(urlt)\n",
    "    ss = cont.decode('ascii')\n",
    "    lines=ss.split('\\r')\n",
    "    toadd=\"\"\n",
    "    pdfs=[x.split('\\t')[2] for x in lines \\\n",
    "          if len(x.strip())>0 and x.split('\\t')[2][-3:]=='pdf']\n",
    "    htms=[x.split('\\t')[2] for x in lines \\\n",
    "          if  len(x.strip())>0 and x.split('\\t')[2][-3:]=='htm']\n",
    "    graphs = [x.split('\\t')[2] for x in lines \\\n",
    "              if len(x.strip())>0 and x.split('\\t')[1]=='GRAPHIC']\n",
    "    if(not graphs): continue\n",
    "        \n",
    "    cnts= [detechString(urlp+\"/\"+htm, graphs) for htm in htms]\n",
    "    #either pdf or the htm that contains all the graph filenames.\n",
    "    toadd= pdfs[0] if pdfs else htms[cnts.index(max(cnts))]\n",
    "\n",
    "    res.add(urlp+\"/\"+toadd)\n",
    "\n",
    "with open(\"data/presentation_urls.txt\", 'w') as outfile:\n",
    "    outfile.write(\"\\r\".join(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One very last thing, save our Random Forest model, we can use it our daily production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/presentation_rfmodel.pkl']"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(clf, \"data/presentation_rfmodel.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
